{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob, random, time, os, zlib\n",
    "import model\n",
    "\n",
    "FEATURE_COUNT = 6 + 6 + 1\n",
    "CROSS_VAL_SIZE = 3000\n",
    "IN_SAMPLE_SIZE = 1000\n",
    "MINIBATCH_SIZE = 256\n",
    "DATA_ROOT = \"build2/\"\n",
    "TOTAL_CHUNK_COUNT = 12\n",
    "\n",
    "def to_hms(x):\n",
    "    x = int(x)\n",
    "    seconds = x % 60\n",
    "    minutes = (x // 60) % 60\n",
    "    hours   = x // 60 // 60\n",
    "    return \"%2i:%02i:%02i\" % (hours, minutes, seconds)\n",
    "\n",
    "# For some reason some Python versions basically explode on .decode(\"zlib\") for large strings.\n",
    "# We can bypass by just decoding it in blocks ourself and assembling them.\n",
    "def stream_decompress(s):\n",
    "    decomp = zlib.decompressobj()\n",
    "    block_size = 2**23\n",
    "    i = 0\n",
    "    results = []\n",
    "    while i < len(s):\n",
    "        block = s[i:i+block_size]\n",
    "        results.append(decomp.decompress(block))\n",
    "        i += block_size\n",
    "    results.append(decomp.flush())\n",
    "    return \"\".join(results)\n",
    "\n",
    "def load_chunk(features, moves):\n",
    "    def load_flat_array(path, shape):\n",
    "        with open(path) as f:\n",
    "            data = f.read()\n",
    "        data = stream_decompress(data)\n",
    "        return np.fromstring(data, dtype=np.int8).reshape(shape)\n",
    "    features = load_flat_array(features, (-1, 8, 8, FEATURE_COUNT))\n",
    "    moves    = load_flat_array(moves, (-1, 8, 8, 2))\n",
    "    # Move each sample to be of shape (2, 8, 8) so we can use tf.nn.softmax_cross_entropy_with_logits_v2.\n",
    "    moves    = np.moveaxis(moves, -1, 1)\n",
    "    assert len(features) == len(moves)\n",
    "    return {\"features\": features, \"moves\": moves}\n",
    "\n",
    "# Views into the extremely large dataset.\n",
    "next_chunk_index = 0\n",
    "chunk = None\n",
    "in_sample_test = None\n",
    "\n",
    "def load_next_chunk():\n",
    "    global next_chunk_index, chunk, in_sample_test\n",
    "    print \"    >>> Loading chunk:\", next_chunk_index\n",
    "    # Free the memory from the previous chunk FIRST, if we have one loaded.\n",
    "    # This is necessary to avoid running out of memory.\n",
    "    if chunk is not None:\n",
    "        del chunk\n",
    "        del in_sample_test\n",
    "    start = time.time()\n",
    "    chunk = load_chunk(\n",
    "        os.path.join(DATA_ROOT, \"features_%03i.z\" % next_chunk_index),\n",
    "        os.path.join(DATA_ROOT, \"moves_%03i.z\" % next_chunk_index),\n",
    "    )\n",
    "    next_chunk_index = (next_chunk_index + 1) % TOTAL_CHUNK_COUNT\n",
    "    in_sample_test = {\n",
    "        \"features\": chunk[\"features\"][:IN_SAMPLE_SIZE],\n",
    "        \"moves\":    chunk[\"moves\"][:IN_SAMPLE_SIZE],\n",
    "    }\n",
    "    stop = time.time()\n",
    "    print \"    >>> (In %f) Samples: %i\" % (stop - start, len(chunk[\"features\"]))\n",
    "\n",
    "def get_random_subset(samples, n):\n",
    "    indices = random.sample(xrange(len(samples[\"features\"])), n)\n",
    "    return {\n",
    "        \"features\": [samples[\"features\"][i] for i in indices],\n",
    "        \"moves\": [samples[\"moves\"][i] for i in indices],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Loading chunk: 0\n",
      "    >>> (In 12.184112) Samples: 6764463\n"
     ]
    }
   ],
   "source": [
    "load_next_chunk()\n",
    "cross_val = load_chunk(\n",
    "    os.path.join(DATA_ROOT, \"test_features.z\"),\n",
    "    os.path.join(DATA_ROOT, \"test_moves.z\"),\n",
    ")\n",
    "cross_val = get_random_subset(cross_val, CROSS_VAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total network parameters: 6728448\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "net = model.ChessNet()\n",
    "print \"Total network parameters:\", net.total_parameters\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "total_training_steps = 0\n",
    "loss_plot = []\n",
    "in_sample_loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m     0 [ 0:00:00 -  0:00:00] Loss: 4.821275  In-sample loss: 4.829871  Accuracy: 0.000  lr = 0.010000\u001b[0m\n",
      "\u001b[31m   500 [ 0:01:21 -  0:01:19] Loss: 3.369675  In-sample loss: 3.427868  Accuracy: 6.333  lr = 0.009957\u001b[0m\n",
      "\u001b[31m  1000 [ 0:02:43 -  0:02:39] Loss: 3.023048  In-sample loss: 3.086771  Accuracy: 8.400  lr = 0.009914\u001b[0m\n",
      "\u001b[31m  1500 [ 0:04:04 -  0:03:58] Loss: 2.820273  In-sample loss: 2.937511  Accuracy: 9.233  lr = 0.009871\u001b[0m\n",
      "\u001b[31m  2000 [ 0:05:26 -  0:05:19] Loss: 2.757127  In-sample loss: 2.858121  Accuracy: 10.967  lr = 0.009828\u001b[0m\n",
      "    >>> Loading chunk: 1\n",
      "    >>> (In 13.072081) Samples: 6736364\n",
      "\u001b[31m  2500 [ 0:07:00 -  0:06:38] Loss: 2.695291  In-sample loss: 2.603695  Accuracy: 10.567  lr = 0.009786\u001b[0m\n",
      "\u001b[31m  3000 [ 0:08:20 -  0:07:57] Loss: 2.673718  In-sample loss: 2.581700  Accuracy: 12.167  lr = 0.009743\u001b[0m\n",
      "\u001b[31m  3500 [ 0:09:44 -  0:09:19] Loss: 2.617728  In-sample loss: 2.522264  Accuracy: 11.700  lr = 0.009701\u001b[0m\n",
      "\u001b[31m  4000 [ 0:11:06 -  0:10:40] Loss: 2.591526  In-sample loss: 2.493384  Accuracy: 12.633  lr = 0.009659\u001b[0m\n",
      "  4500 [ 0:12:28 -  0:12:00] Loss: 2.591962  In-sample loss: 2.470532  Accuracy: 12.233  lr = 0.009618\n",
      "    >>> Loading chunk: 2\n",
      "    >>> (In 12.929109) Samples: 6739494\n",
      "\u001b[31m  5000 [ 0:14:02 -  0:13:20] Loss: 2.547672  In-sample loss: 2.423707  Accuracy: 13.633  lr = 0.009576\u001b[0m\n",
      "  5500 [ 0:15:24 -  0:14:40] Loss: 2.547879  In-sample loss: 2.405254  Accuracy: 12.400  lr = 0.009535\n",
      "  6000 [ 0:16:45 -  0:16:00] Loss: 2.564728  In-sample loss: 2.447409  Accuracy: 11.333  lr = 0.009493\n",
      "\u001b[31m  6500 [ 0:18:07 -  0:17:19] Loss: 2.520356  In-sample loss: 2.382981  Accuracy: 13.600  lr = 0.009452\u001b[0m\n",
      "\u001b[31m  7000 [ 0:19:29 -  0:18:40] Loss: 2.463840  In-sample loss: 2.357383  Accuracy: 13.867  lr = 0.009412\u001b[0m\n",
      "    >>> Loading chunk: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9a9b7d8972ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Periodically swap out the data for fresh training data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moverall_step\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mload_next_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m#    if (overall_step + 1) % 20 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#        save_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-fe98313239f5>\u001b[0m in \u001b[0;36mload_next_chunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     chunk = load_chunk(\n\u001b[1;32m     64\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"features_%03i.z\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnext_chunk_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"moves_%03i.z\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnext_chunk_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     )\n\u001b[1;32m     67\u001b[0m     \u001b[0mnext_chunk_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnext_chunk_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mTOTAL_CHUNK_COUNT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-fe98313239f5>\u001b[0m in \u001b[0;36mload_chunk\u001b[0;34m(features, moves)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream_decompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_flat_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEATURE_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mmoves\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mload_flat_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Move each sample to be of shape (2, 8, 8) so we can use tf.nn.softmax_cross_entropy_with_logits_v2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-fe98313239f5>\u001b[0m in \u001b[0;36mload_flat_array\u001b[0;34m(path, shape)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream_decompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_flat_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEATURE_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-fe98313239f5>\u001b[0m in \u001b[0;36mstream_decompress\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_work = 0.0\n",
    "start_time = time.time()\n",
    "best_loss = float(\"inf\")\n",
    "lr_schedule = lambda step: 0.01 * 0.5**(step / 8e4)\n",
    "\n",
    "for overall_step in range(10000):\n",
    "    lr = lr_schedule(total_training_steps)\n",
    "    elapsed = time.time() - start_time\n",
    "    in_sample_loss = net.get_loss(in_sample_test)\n",
    "    loss = net.get_loss(cross_val)\n",
    "    color_pair = \"\", \"\"\n",
    "    if loss < best_loss:\n",
    "        color_pair = \"\\x1b[31m\", \"\\x1b[0m\"\n",
    "    message = \"%s%6i [%s - %s] Loss: %.6f  In-sample loss: %.6f  Accuracy: %.3f  lr = %f%s\" % (\n",
    "        color_pair[0],\n",
    "        total_training_steps,\n",
    "        to_hms(elapsed),\n",
    "        to_hms(total_work),\n",
    "        loss,\n",
    "        in_sample_loss,\n",
    "        net.get_accuracy(cross_val) * 100,\n",
    "        lr,\n",
    "        color_pair[1]\n",
    "    )\n",
    "    print(message)\n",
    "    with open(\"/home/snp/chess_training_log\", \"a+\") as f:\n",
    "        print >>f, message\n",
    "    loss_plot.append((total_training_steps, loss))\n",
    "    in_sample_loss_plot.append((total_training_steps, in_sample_loss))\n",
    "    best_loss = min(best_loss, loss)\n",
    "\n",
    "    for _ in range(500):\n",
    "        minibatch = get_random_subset(chunk, MINIBATCH_SIZE)\n",
    "        working = time.time()\n",
    "        net.train(minibatch, lr)\n",
    "        total_work += time.time() - working\n",
    "        # Try really hard to not keep any views around!\n",
    "        del minibatch\n",
    "        total_training_steps += 1\n",
    "\n",
    "    # Periodically swap out the data for fresh training data.\n",
    "    if (overall_step + 1) % 5 == 0:\n",
    "        load_next_chunk()\n",
    "#    if (overall_step + 1) % 20 == 0:\n",
    "#        save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sess = sess\n",
    "model.save_model(net, \"model.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [12, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hold(True)\n",
    "plt.plot(*zip(*loss_plot[6:]))\n",
    "plt.plot(*zip(*in_sample_loss_plot[6:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob, random, time, os, zlib\n",
    "import model\n",
    "\n",
    "FEATURE_COUNT = 6 + 6 + 1\n",
    "CROSS_VAL_SIZE = 3000\n",
    "IN_SAMPLE_SIZE = 1000\n",
    "MINIBATCH_SIZE = 256\n",
    "DATA_ROOT = \"build2/\"\n",
    "TOTAL_CHUNK_COUNT = 12\n",
    "\n",
    "def to_hms(x):\n",
    "    x = int(x)\n",
    "    seconds = x % 60\n",
    "    minutes = (x // 60) % 60\n",
    "    hours   = x // 60 // 60\n",
    "    return \"%2i:%02i:%02i\" % (hours, minutes, seconds)\n",
    "\n",
    "# For some reason some Python versions basically explode on .decode(\"zlib\") for large strings.\n",
    "# We can bypass by just decoding it in blocks ourself and assembling them.\n",
    "def stream_decompress(s):\n",
    "    decomp = zlib.decompressobj()\n",
    "    block_size = 2**23\n",
    "    i = 0\n",
    "    results = []\n",
    "    while i < len(s):\n",
    "        block = s[i:i+block_size]\n",
    "        results.append(decomp.decompress(block))\n",
    "        i += block_size\n",
    "    results.append(decomp.flush())\n",
    "    return \"\".join(results)\n",
    "\n",
    "def load_chunk(features, moves):\n",
    "    def load_flat_array(path, shape):\n",
    "        with open(path) as f:\n",
    "            data = f.read()\n",
    "        data = stream_decompress(data)\n",
    "        return np.fromstring(data, dtype=np.int8).reshape(shape)\n",
    "    features = load_flat_array(features, (-1, 8, 8, FEATURE_COUNT))\n",
    "    moves    = load_flat_array(moves, (-1, 8, 8, 2))\n",
    "    # Move each sample to be of shape (2, 8, 8) so we can use tf.nn.softmax_cross_entropy_with_logits_v2.\n",
    "    moves    = np.moveaxis(moves, -1, 1)\n",
    "    assert len(features) == len(moves)\n",
    "    return {\"features\": features, \"moves\": moves}\n",
    "\n",
    "# Views into the extremely large dataset.\n",
    "next_chunk_index = 0\n",
    "chunk = None\n",
    "in_sample_test = None\n",
    "\n",
    "def load_next_chunk():\n",
    "    global next_chunk_index, chunk, in_sample_test\n",
    "    print \"    >>> Loading chunk:\", next_chunk_index\n",
    "    # Free the memory from the previous chunk FIRST, if we have one loaded.\n",
    "    # This is necessary to avoid running out of memory.\n",
    "    if chunk is not None:\n",
    "        del chunk\n",
    "        del in_sample_test\n",
    "    start = time.time()\n",
    "    chunk = load_chunk(\n",
    "        os.path.join(DATA_ROOT, \"features_%03i.z\" % next_chunk_index),\n",
    "        os.path.join(DATA_ROOT, \"moves_%03i.z\" % next_chunk_index),\n",
    "    )\n",
    "    next_chunk_index = (next_chunk_index + 1) % TOTAL_CHUNK_COUNT\n",
    "    in_sample_test = {\n",
    "        \"features\": chunk[\"features\"][:IN_SAMPLE_SIZE],\n",
    "        \"moves\":    chunk[\"moves\"][:IN_SAMPLE_SIZE],\n",
    "    }\n",
    "    stop = time.time()\n",
    "    print \"    >>> (In %f) Samples: %i\" % (stop - start, len(chunk[\"features\"]))\n",
    "\n",
    "def get_random_subset(samples, n):\n",
    "    indices = random.sample(xrange(len(samples[\"features\"])), n)\n",
    "    return {\n",
    "        \"features\": [samples[\"features\"][i] for i in indices],\n",
    "        \"moves\": [samples[\"moves\"][i] for i in indices],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Loading chunk: 0\n",
      "    >>> (In 12.563726) Samples: 6764463\n"
     ]
    }
   ],
   "source": [
    "load_next_chunk()\n",
    "cross_val = load_chunk(\n",
    "    os.path.join(DATA_ROOT, \"test_features.z\"),\n",
    "    os.path.join(DATA_ROOT, \"test_moves.z\"),\n",
    ")\n",
    "cross_val = get_random_subset(cross_val, CROSS_VAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total network parameters: 2797312\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "net = model.ChessNet()\n",
    "print \"Total network parameters:\", net.total_parameters\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "total_training_steps = 0\n",
    "loss_plot = []\n",
    "in_sample_loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m     0 [ 0:00:00 -  0:00:00] Loss: 4.245204  In-sample loss: 4.241312  Accuracy: 0.000  lr = 0.010000\u001b[0m\n",
      "\u001b[31m   500 [ 0:00:25 -  0:00:23] Loss: 3.501557  In-sample loss: 3.587129  Accuracy: 6.467  lr = 0.009957\u001b[0m\n",
      "\u001b[31m  1000 [ 0:00:49 -  0:00:46] Loss: 3.212693  In-sample loss: 3.320307  Accuracy: 6.800  lr = 0.009914\u001b[0m\n",
      "\u001b[31m  1500 [ 0:01:12 -  0:01:08] Loss: 3.013221  In-sample loss: 3.118905  Accuracy: 7.233  lr = 0.009871\u001b[0m\n",
      "\u001b[31m  2000 [ 0:01:36 -  0:01:31] Loss: 2.872252  In-sample loss: 2.964665  Accuracy: 9.667  lr = 0.009828\u001b[0m\n",
      "    >>> Loading chunk: 1\n",
      "    >>> (In 12.591915) Samples: 6736364\n",
      "\u001b[31m  2500 [ 0:02:12 -  0:01:54] Loss: 2.797369  In-sample loss: 2.764048  Accuracy: 10.333  lr = 0.009786\u001b[0m\n",
      "\u001b[31m  3000 [ 0:02:35 -  0:02:17] Loss: 2.781773  In-sample loss: 2.717456  Accuracy: 10.300  lr = 0.009743\u001b[0m\n",
      "\u001b[31m  3500 [ 0:02:58 -  0:02:39] Loss: 2.727601  In-sample loss: 2.628811  Accuracy: 10.300  lr = 0.009701\u001b[0m\n",
      "\u001b[31m  4000 [ 0:03:21 -  0:03:02] Loss: 2.673885  In-sample loss: 2.621279  Accuracy: 11.533  lr = 0.009659\u001b[0m\n",
      "\u001b[31m  4500 [ 0:03:44 -  0:03:25] Loss: 2.643626  In-sample loss: 2.572691  Accuracy: 12.767  lr = 0.009618\u001b[0m\n",
      "    >>> Loading chunk: 2\n",
      "    >>> (In 12.369061) Samples: 6739494\n",
      "\u001b[31m  5000 [ 0:04:20 -  0:03:47] Loss: 2.624552  In-sample loss: 2.513002  Accuracy: 11.600  lr = 0.009576\u001b[0m\n",
      "\u001b[31m  5500 [ 0:04:43 -  0:04:10] Loss: 2.581267  In-sample loss: 2.493405  Accuracy: 12.833  lr = 0.009535\u001b[0m\n",
      "\u001b[31m  6000 [ 0:05:06 -  0:04:33] Loss: 2.566789  In-sample loss: 2.472106  Accuracy: 13.033  lr = 0.009493\u001b[0m\n",
      "  6500 [ 0:05:29 -  0:04:55] Loss: 2.583130  In-sample loss: 2.509568  Accuracy: 12.867  lr = 0.009452\n",
      "  7000 [ 0:05:53 -  0:05:18] Loss: 2.574515  In-sample loss: 2.456732  Accuracy: 13.667  lr = 0.009412\n",
      "    >>> Loading chunk: 3\n",
      "    >>> (In 12.382536) Samples: 6737983\n",
      "\u001b[31m  7500 [ 0:06:28 -  0:05:41] Loss: 2.534218  In-sample loss: 2.549853  Accuracy: 13.933  lr = 0.009371\u001b[0m\n",
      "\u001b[31m  8000 [ 0:06:51 -  0:06:03] Loss: 2.519626  In-sample loss: 2.534248  Accuracy: 14.533  lr = 0.009330\u001b[0m\n",
      "\u001b[31m  8500 [ 0:07:15 -  0:06:26] Loss: 2.512433  In-sample loss: 2.488676  Accuracy: 13.600  lr = 0.009290\u001b[0m\n",
      "\u001b[31m  9000 [ 0:07:38 -  0:06:48] Loss: 2.496484  In-sample loss: 2.485572  Accuracy: 14.733  lr = 0.009250\u001b[0m\n",
      "\u001b[31m  9500 [ 0:08:01 -  0:07:11] Loss: 2.495046  In-sample loss: 2.507313  Accuracy: 14.000  lr = 0.009210\u001b[0m\n",
      "    >>> Loading chunk: 4\n",
      "    >>> (In 12.346571) Samples: 6759446\n",
      "\u001b[31m 10000 [ 0:08:37 -  0:07:34] Loss: 2.479621  In-sample loss: 2.579926  Accuracy: 15.100  lr = 0.009170\u001b[0m\n",
      " 10500 [ 0:09:00 -  0:07:56] Loss: 2.541080  In-sample loss: 2.619233  Accuracy: 12.967  lr = 0.009130\n",
      "\u001b[31m 11000 [ 0:09:23 -  0:08:19] Loss: 2.467772  In-sample loss: 2.550582  Accuracy: 13.267  lr = 0.009091\u001b[0m\n",
      "\u001b[31m 11500 [ 0:09:46 -  0:08:42] Loss: 2.434420  In-sample loss: 2.545265  Accuracy: 15.233  lr = 0.009052\u001b[0m\n",
      " 12000 [ 0:10:09 -  0:09:04] Loss: 2.439395  In-sample loss: 2.500658  Accuracy: 16.000  lr = 0.009013\n",
      "    >>> Loading chunk: 5\n",
      "    >>> (In 12.342670) Samples: 6739228\n",
      " 12500 [ 0:10:45 -  0:09:27] Loss: 2.441476  In-sample loss: 2.439613  Accuracy: 15.633  lr = 0.008974\n",
      "\u001b[31m 13000 [ 0:11:08 -  0:09:49] Loss: 2.420719  In-sample loss: 2.430094  Accuracy: 14.833  lr = 0.008935\u001b[0m\n",
      " 13500 [ 0:11:31 -  0:10:12] Loss: 2.515579  In-sample loss: 2.523657  Accuracy: 14.033  lr = 0.008896\n",
      "\u001b[31m 14000 [ 0:11:54 -  0:10:35] Loss: 2.407037  In-sample loss: 2.363940  Accuracy: 14.933  lr = 0.008858\u001b[0m\n",
      "\u001b[31m 14500 [ 0:12:17 -  0:10:57] Loss: 2.396734  In-sample loss: 2.406847  Accuracy: 15.633  lr = 0.008819\u001b[0m\n",
      "    >>> Loading chunk: 6\n",
      "    >>> (In 12.324152) Samples: 6755400\n",
      " 15000 [ 0:12:53 -  0:11:20] Loss: 2.409963  In-sample loss: 2.237892  Accuracy: 15.733  lr = 0.008781\n",
      "\u001b[31m 15500 [ 0:13:16 -  0:11:43] Loss: 2.381138  In-sample loss: 2.195478  Accuracy: 16.167  lr = 0.008743\u001b[0m\n",
      " 16000 [ 0:13:39 -  0:12:05] Loss: 2.386090  In-sample loss: 2.188830  Accuracy: 15.000  lr = 0.008706\n",
      " 16500 [ 0:14:02 -  0:12:28] Loss: 2.434244  In-sample loss: 2.233140  Accuracy: 15.300  lr = 0.008668\n",
      "\u001b[31m 17000 [ 0:14:26 -  0:12:51] Loss: 2.374105  In-sample loss: 2.167077  Accuracy: 16.433  lr = 0.008630\u001b[0m\n",
      "    >>> Loading chunk: 7\n",
      "    >>> (In 12.385068) Samples: 6749431\n",
      " 17500 [ 0:15:01 -  0:13:13] Loss: 2.406359  In-sample loss: 2.354019  Accuracy: 14.633  lr = 0.008593\n",
      "\u001b[31m 18000 [ 0:15:24 -  0:13:36] Loss: 2.355990  In-sample loss: 2.334600  Accuracy: 16.800  lr = 0.008556\u001b[0m\n",
      "\u001b[31m 18500 [ 0:15:48 -  0:13:58] Loss: 2.346825  In-sample loss: 2.332197  Accuracy: 17.133  lr = 0.008519\u001b[0m\n",
      " 19000 [ 0:16:11 -  0:14:21] Loss: 2.388981  In-sample loss: 2.352143  Accuracy: 15.667  lr = 0.008482\n",
      " 19500 [ 0:16:34 -  0:14:44] Loss: 2.359949  In-sample loss: 2.346078  Accuracy: 16.633  lr = 0.008445\n",
      "    >>> Loading chunk: 8\n",
      "    >>> (In 12.267531) Samples: 6748371\n",
      "\u001b[31m 20000 [ 0:17:10 -  0:15:06] Loss: 2.341431  In-sample loss: 2.235335  Accuracy: 15.567  lr = 0.008409\u001b[0m\n",
      "\u001b[31m 20500 [ 0:17:33 -  0:15:29] Loss: 2.323708  In-sample loss: 2.201947  Accuracy: 17.533  lr = 0.008373\u001b[0m\n",
      " 21000 [ 0:17:56 -  0:15:52] Loss: 2.415342  In-sample loss: 2.236237  Accuracy: 15.767  lr = 0.008336\n",
      " 21500 [ 0:18:19 -  0:16:14] Loss: 2.324389  In-sample loss: 2.190771  Accuracy: 17.733  lr = 0.008300\n",
      " 22000 [ 0:18:42 -  0:16:37] Loss: 2.324027  In-sample loss: 2.186314  Accuracy: 17.167  lr = 0.008265\n",
      "    >>> Loading chunk: 9\n",
      "    >>> (In 12.341368) Samples: 6737514\n",
      " 22500 [ 0:19:18 -  0:17:00] Loss: 2.330110  In-sample loss: 2.293783  Accuracy: 17.100  lr = 0.008229\n",
      "\u001b[31m 23000 [ 0:19:41 -  0:17:22] Loss: 2.286502  In-sample loss: 2.275812  Accuracy: 17.333  lr = 0.008193\u001b[0m\n",
      " 23500 [ 0:20:04 -  0:17:45] Loss: 2.537672  In-sample loss: 2.466470  Accuracy: 13.233  lr = 0.008158\n",
      " 24000 [ 0:20:27 -  0:18:08] Loss: 2.310361  In-sample loss: 2.296315  Accuracy: 17.133  lr = 0.008123\n",
      " 24500 [ 0:20:51 -  0:18:30] Loss: 2.305659  In-sample loss: 2.262394  Accuracy: 17.800  lr = 0.008087\n",
      "    >>> Loading chunk: 10\n",
      "    >>> (In 12.358136) Samples: 6737041\n",
      " 25000 [ 0:21:26 -  0:18:53] Loss: 2.323483  In-sample loss: 2.257907  Accuracy: 17.833  lr = 0.008052\n",
      "\u001b[31m 25500 [ 0:21:49 -  0:19:15] Loss: 2.281277  In-sample loss: 2.214390  Accuracy: 17.833  lr = 0.008018\u001b[0m\n",
      "\u001b[31m 26000 [ 0:22:12 -  0:19:38] Loss: 2.276113  In-sample loss: 2.251700  Accuracy: 16.533  lr = 0.007983\u001b[0m\n",
      " 26500 [ 0:22:36 -  0:20:01] Loss: 2.298370  In-sample loss: 2.234506  Accuracy: 17.800  lr = 0.007948\n",
      " 27000 [ 0:22:59 -  0:20:23] Loss: 2.298771  In-sample loss: 2.217360  Accuracy: 17.733  lr = 0.007914\n",
      "    >>> Loading chunk: 11\n",
      "    >>> (In 12.410496) Samples: 6755215\n",
      " 27500 [ 0:23:35 -  0:20:46] Loss: 2.280758  In-sample loss: 2.219353  Accuracy: 18.267  lr = 0.007880\n",
      " 28000 [ 0:23:58 -  0:21:09] Loss: 2.279853  In-sample loss: 2.221115  Accuracy: 18.300  lr = 0.007846\n",
      " 28500 [ 0:24:21 -  0:21:31] Loss: 2.297841  In-sample loss: 2.245234  Accuracy: 17.467  lr = 0.007812\n",
      " 29000 [ 0:24:44 -  0:21:54] Loss: 2.315840  In-sample loss: 2.255440  Accuracy: 17.700  lr = 0.007778\n",
      " 29500 [ 0:25:07 -  0:22:17] Loss: 2.291545  In-sample loss: 2.215135  Accuracy: 18.067  lr = 0.007745\n",
      "    >>> Loading chunk: 0\n",
      "    >>> (In 12.454393) Samples: 6764463\n",
      "\u001b[31m 30000 [ 0:25:43 -  0:22:39] Loss: 2.260742  In-sample loss: 2.384108  Accuracy: 18.433  lr = 0.007711\u001b[0m\n",
      " 30500 [ 0:26:06 -  0:23:02] Loss: 2.287746  In-sample loss: 2.366213  Accuracy: 18.333  lr = 0.007678\n",
      " 31000 [ 0:26:29 -  0:23:24] Loss: 2.371668  In-sample loss: 2.499710  Accuracy: 16.867  lr = 0.007645\n",
      "\u001b[31m 31500 [ 0:26:52 -  0:23:47] Loss: 2.254092  In-sample loss: 2.362192  Accuracy: 18.600  lr = 0.007611\u001b[0m\n",
      "\u001b[31m 32000 [ 0:27:16 -  0:24:10] Loss: 2.234373  In-sample loss: 2.325135  Accuracy: 18.300  lr = 0.007579\u001b[0m\n",
      "    >>> Loading chunk: 1\n",
      "    >>> (In 12.394421) Samples: 6736364\n",
      " 32500 [ 0:27:51 -  0:24:32] Loss: 2.237096  In-sample loss: 2.160141  Accuracy: 18.933  lr = 0.007546\n",
      "\u001b[31m 33000 [ 0:28:14 -  0:24:55] Loss: 2.228225  In-sample loss: 2.152849  Accuracy: 19.400  lr = 0.007513\u001b[0m\n",
      " 33500 [ 0:28:38 -  0:25:18] Loss: 2.292368  In-sample loss: 2.231418  Accuracy: 17.800  lr = 0.007481\n",
      " 34000 [ 0:29:01 -  0:25:40] Loss: 2.240764  In-sample loss: 2.178655  Accuracy: 19.300  lr = 0.007448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m 34500 [ 0:29:24 -  0:26:03] Loss: 2.215088  In-sample loss: 2.162228  Accuracy: 18.167  lr = 0.007416\u001b[0m\n",
      "    >>> Loading chunk: 2\n",
      "    >>> (In 12.348594) Samples: 6739494\n",
      " 35000 [ 0:30:00 -  0:26:26] Loss: 2.249152  In-sample loss: 2.159724  Accuracy: 18.167  lr = 0.007384\n",
      "\u001b[31m 35500 [ 0:30:23 -  0:26:48] Loss: 2.206133  In-sample loss: 2.131828  Accuracy: 19.800  lr = 0.007352\u001b[0m\n",
      " 36000 [ 0:30:46 -  0:27:11] Loss: 2.224047  In-sample loss: 2.110447  Accuracy: 18.767  lr = 0.007320\n",
      " 36500 [ 0:31:09 -  0:27:34] Loss: 2.222814  In-sample loss: 2.105618  Accuracy: 18.767  lr = 0.007289\n",
      " 37000 [ 0:31:32 -  0:27:56] Loss: 2.215075  In-sample loss: 2.122285  Accuracy: 19.267  lr = 0.007257\n",
      "    >>> Loading chunk: 3\n",
      "    >>> (In 12.439680) Samples: 6737983\n",
      " 37500 [ 0:32:08 -  0:28:19] Loss: 2.288257  In-sample loss: 2.324234  Accuracy: 18.633  lr = 0.007226\n",
      "\u001b[31m 38000 [ 0:32:31 -  0:28:41] Loss: 2.201880  In-sample loss: 2.235601  Accuracy: 19.900  lr = 0.007195\u001b[0m\n",
      " 38500 [ 0:32:54 -  0:29:04] Loss: 2.223922  In-sample loss: 2.225298  Accuracy: 18.100  lr = 0.007164\n",
      " 39000 [ 0:33:17 -  0:29:27] Loss: 2.212034  In-sample loss: 2.204549  Accuracy: 18.567  lr = 0.007133\n",
      "\u001b[31m 39500 [ 0:33:41 -  0:29:49] Loss: 2.185894  In-sample loss: 2.215109  Accuracy: 19.800  lr = 0.007102\u001b[0m\n",
      "    >>> Loading chunk: 4\n",
      "    >>> (In 12.401394) Samples: 6759446\n",
      " 40000 [ 0:34:16 -  0:30:12] Loss: 2.274285  In-sample loss: 2.389535  Accuracy: 17.633  lr = 0.007071\n",
      " 40500 [ 0:34:39 -  0:30:35] Loss: 2.189531  In-sample loss: 2.269056  Accuracy: 19.300  lr = 0.007041\n",
      " 41000 [ 0:35:03 -  0:30:57] Loss: 2.203723  In-sample loss: 2.297853  Accuracy: 18.800  lr = 0.007010\n",
      " 41500 [ 0:35:26 -  0:31:20] Loss: 2.188697  In-sample loss: 2.245285  Accuracy: 18.867  lr = 0.006980\n",
      " 42000 [ 0:35:50 -  0:31:44] Loss: 2.191721  In-sample loss: 2.264620  Accuracy: 19.433  lr = 0.006950\n",
      "    >>> Loading chunk: 5\n",
      "    >>> (In 12.533846) Samples: 6739228\n",
      " 42500 [ 0:36:27 -  0:32:07] Loss: 2.194684  In-sample loss: 2.163395  Accuracy: 20.033  lr = 0.006920\n",
      " 43000 [ 0:36:50 -  0:32:30] Loss: 2.186684  In-sample loss: 2.204180  Accuracy: 19.933  lr = 0.006890\n",
      " 43500 [ 0:37:14 -  0:32:53] Loss: 2.232156  In-sample loss: 2.228710  Accuracy: 19.167  lr = 0.006860\n",
      "\u001b[31m 44000 [ 0:37:38 -  0:33:17] Loss: 2.180160  In-sample loss: 2.176641  Accuracy: 19.200  lr = 0.006830\u001b[0m\n",
      " 44500 [ 0:38:01 -  0:33:40] Loss: 2.200026  In-sample loss: 2.164162  Accuracy: 19.500  lr = 0.006801\n",
      "    >>> Loading chunk: 6\n",
      "    >>> (In 12.742377) Samples: 6755400\n",
      "\u001b[31m 45000 [ 0:38:38 -  0:34:03] Loss: 2.179590  In-sample loss: 1.968962  Accuracy: 19.567  lr = 0.006771\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9a9b7d8972ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mminibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMINIBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mworking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_work\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mworking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Try really hard to not keep any views around!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/snp/proj/TinyMLChess/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, samples, learning_rate)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_on_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/snp/proj/TinyMLChess/model.py\u001b[0m in \u001b[0;36mrun_on_samples\u001b[0;34m(self, f, samples, learning_rate, is_training)\u001b[0m\n\u001b[1;32m    137\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesired_output_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate_ph\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training_ph\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \t\t})\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   2211\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m     \"\"\"\n\u001b[0;32m-> 2213\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4813\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4814\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4815\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_work = 0.0\n",
    "start_time = time.time()\n",
    "best_loss = float(\"inf\")\n",
    "lr_schedule = lambda step: 0.01 * 0.5**(step / 8e4)\n",
    "\n",
    "for overall_step in range(10000):\n",
    "    lr = lr_schedule(total_training_steps)\n",
    "    elapsed = time.time() - start_time\n",
    "    in_sample_loss = net.get_loss(in_sample_test)\n",
    "    loss = net.get_loss(cross_val)\n",
    "    color_pair = \"\", \"\"\n",
    "    if loss < best_loss:\n",
    "        color_pair = \"\\x1b[31m\", \"\\x1b[0m\"\n",
    "    message = \"%s%6i [%s - %s] Loss: %.6f  In-sample loss: %.6f  Accuracy: %.3f  lr = %f%s\" % (\n",
    "        color_pair[0],\n",
    "        total_training_steps,\n",
    "        to_hms(elapsed),\n",
    "        to_hms(total_work),\n",
    "        loss,\n",
    "        in_sample_loss,\n",
    "        net.get_accuracy(cross_val) * 100,\n",
    "        lr,\n",
    "        color_pair[1]\n",
    "    )\n",
    "    print(message)\n",
    "    with open(\"/home/snp/chess_training_log\", \"a+\") as f:\n",
    "        print >>f, message\n",
    "    loss_plot.append((total_training_steps, loss))\n",
    "    in_sample_loss_plot.append((total_training_steps, in_sample_loss))\n",
    "    best_loss = min(best_loss, loss)\n",
    "\n",
    "    for _ in range(500):\n",
    "        minibatch = get_random_subset(chunk, MINIBATCH_SIZE)\n",
    "        working = time.time()\n",
    "        net.train(minibatch, lr)\n",
    "        total_work += time.time() - working\n",
    "        # Try really hard to not keep any views around!\n",
    "        del minibatch\n",
    "        total_training_steps += 1\n",
    "\n",
    "    # Periodically swap out the data for fresh training data.\n",
    "    if (overall_step + 1) % 5 == 0:\n",
    "        load_next_chunk()\n",
    "#    if (overall_step + 1) % 20 == 0:\n",
    "#        save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

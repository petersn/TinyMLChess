{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = lambda x, y: x + y * 8\n",
    "queen_moves = sum(\n",
    "    ([\n",
    "        D(0, i), D(i, 0),\n",
    "        D(0, -i), D(-i, 0),\n",
    "        D(i,  i), D(-i,  i),\n",
    "        D(i, -i), D(-i, -i),\n",
    "    ]\n",
    "    for i in xrange(1, 8)),\n",
    "    []\n",
    ")\n",
    "knight_moves = [\n",
    "    D( 1, 2), D( 1, -2),\n",
    "    D(-1, 2), D(-1, -2),\n",
    "    D( 2, 1), D( 2, -1),\n",
    "    D(-2, 1), D(-2, -1),\n",
    "]\n",
    "all_layers = sorted(queen_moves + knight_moves)\n",
    "assert len(all_layers) == 64\n",
    "difference_to_layer_index = {diff: i for i, diff in enumerate(all_layers)}\n",
    "\n",
    "def one_hot_to_large(move):\n",
    "#    assert move.shape == (1,)\n",
    "#    return move\n",
    "    assert move.shape == (2, 8, 8)\n",
    "    pick_up, put_down = map(np.argmax, move)\n",
    "    difference = put_down - pick_up\n",
    "    result = np.zeros((len(all_layers), 64))\n",
    "    result[difference_to_layer_index[difference], pick_up] = 1\n",
    "    return result.reshape((len(all_layers), 8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob, random, time, os, zlib\n",
    "import model_dual as model\n",
    "\n",
    "FEATURE_COUNT = 6 + 6 + 1\n",
    "CROSS_VAL_SIZE = 3000\n",
    "IN_SAMPLE_SIZE = 1000\n",
    "MINIBATCH_SIZE = 512\n",
    "DATA_ROOT = \"final_output/\"\n",
    "TOTAL_CHUNK_COUNT = 22\n",
    "\n",
    "def to_hms(x):\n",
    "    x = int(x)\n",
    "    seconds = x % 60\n",
    "    minutes = (x // 60) % 60\n",
    "    hours   = x // 60 // 60\n",
    "    return \"%2i:%02i:%02i\" % (hours, minutes, seconds)\n",
    "\n",
    "# For some reason some Python versions basically explode on .decode(\"zlib\") for large strings.\n",
    "# We can bypass by just decoding it in blocks ourself and assembling them.\n",
    "def stream_decompress(s):\n",
    "    decomp = zlib.decompressobj()\n",
    "    block_size = 2**23\n",
    "    i = 0\n",
    "    results = []\n",
    "    while i < len(s):\n",
    "        block = s[i:i+block_size]\n",
    "        results.append(decomp.decompress(block))\n",
    "        i += block_size\n",
    "    results.append(decomp.flush())\n",
    "    return \"\".join(results)\n",
    "\n",
    "def load_chunk(features, moves):\n",
    "    def load_flat_array(path, shape):\n",
    "        with open(path) as f:\n",
    "            data = f.read()\n",
    "        data = stream_decompress(data)\n",
    "        return np.fromstring(data, dtype=np.int8).reshape(shape)\n",
    "    features = load_flat_array(features, (-1, 8, 8, FEATURE_COUNT))\n",
    "    moves    = load_flat_array(moves, (-1, 2, 8, 8))\n",
    "#    # Move each sample to be of shape (2, 8, 8) so we can use tf.nn.softmax_cross_entropy_with_logits_v2.\n",
    "#   moves    = np.moveaxis(moves, -1, 1)\n",
    "    assert len(features) == len(moves)\n",
    "    return {\"features\": features, \"moves\": moves}\n",
    "\n",
    "# Views into the extremely large dataset.\n",
    "next_chunk_index = 0\n",
    "chunk = None\n",
    "in_sample_test = None\n",
    "\n",
    "def load_next_chunk():\n",
    "    global next_chunk_index, chunk, in_sample_test\n",
    "    print \"    >>> Loading chunk:\", next_chunk_index\n",
    "    # Free the memory from the previous chunk FIRST, if we have one loaded.\n",
    "    # This is necessary to avoid running out of memory.\n",
    "    if chunk is not None:\n",
    "        del chunk\n",
    "        del in_sample_test\n",
    "    start = time.time()\n",
    "    chunk = load_chunk(\n",
    "        os.path.join(DATA_ROOT, \"features_%03i.z\" % next_chunk_index),\n",
    "        os.path.join(DATA_ROOT, \"moves_%03i.z\" % next_chunk_index),\n",
    "    )\n",
    "    next_chunk_index = (next_chunk_index + 1) % TOTAL_CHUNK_COUNT\n",
    "    in_sample_test = {\n",
    "        \"features\": chunk[\"features\"][:IN_SAMPLE_SIZE],\n",
    "        \"moves\":    map(one_hot_to_large, chunk[\"moves\"][:IN_SAMPLE_SIZE]),\n",
    "    }\n",
    "    stop = time.time()\n",
    "    print \"    >>> (In %f) Samples: %i\" % (stop - start, len(chunk[\"features\"]))\n",
    "\n",
    "def get_random_subset(samples, n):\n",
    "    indices = random.sample(xrange(len(samples[\"features\"])), n)\n",
    "    return {\n",
    "        \"features\": [samples[\"features\"][i] for i in indices],\n",
    "        \"moves\": [one_hot_to_large(samples[\"moves\"][i]) for i in indices],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    >>> Loading chunk: 0\n",
      "    >>> (In 10.340097) Samples: 7840136\n"
     ]
    }
   ],
   "source": [
    "load_next_chunk()\n",
    "cross_val = load_chunk(\n",
    "    os.path.join(DATA_ROOT, \"test_features.z\"),\n",
    "    os.path.join(DATA_ROOT, \"test_moves.z\"),\n",
    ")\n",
    "cross_val = get_random_subset(cross_val, CROSS_VAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total network parameters: 2972352\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "net = model.ChessPolicyNet(\"policy/\")\n",
    "print \"Total network parameters:\", net.total_parameters\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "total_training_steps = 0\n",
    "loss_plot = []\n",
    "in_sample_loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 64, 64)\n",
      "(?, 64, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "print net.final_output.shape\n",
    "print net.desired_output_ph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_counter = 0\n",
    "def save_model():\n",
    "    global model_save_counter\n",
    "    model_save_counter += 1\n",
    "    model.sess = sess\n",
    "    model.save_model(net, \"models/policy-10x128-model-%03i.npy\" % model_save_counter)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.sess = sess\n",
    "#model.load_model(net, \"models/value-6x128-model-001.npy\")\n",
    "model.load_model(net, \"models/joint-model-040.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m     0 [ 0:00:00 -  0:00:00] Loss: 8.317173  In-sample loss: 8.316814  Accuracy: 0.400  lr = 0.005000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "total_work = 0.0\n",
    "start_time = time.time()\n",
    "best_loss = float(\"inf\")\n",
    "lr_schedule = lambda step: 0.005 * 0.5**(step / 15e4)\n",
    "\n",
    "for overall_step in range(10000):\n",
    "    lr = lr_schedule(total_training_steps)\n",
    "    elapsed = time.time() - start_time\n",
    "    in_sample_loss = net.get_loss(in_sample_test)\n",
    "    loss = net.get_loss(cross_val)\n",
    "    color_pair = \"\", \"\"\n",
    "    if loss < best_loss:\n",
    "        color_pair = \"\\x1b[31m\", \"\\x1b[0m\"\n",
    "    message = \"%s%6i [%s - %s] Loss: %.6f  In-sample loss: %.6f  Accuracy: %.3f  lr = %f%s\" % (\n",
    "        color_pair[0],\n",
    "        total_training_steps,\n",
    "        to_hms(elapsed),\n",
    "        to_hms(total_work),\n",
    "        loss,\n",
    "        in_sample_loss,\n",
    "        net.get_accuracy(cross_val) * 100,\n",
    "        lr,\n",
    "        color_pair[1]\n",
    "    )\n",
    "    print(message)\n",
    "    with open(\"/home/snp/chess_training_log\", \"a+\") as f:\n",
    "        print >>f, message\n",
    "    loss_plot.append((total_training_steps, loss))\n",
    "    in_sample_loss_plot.append((total_training_steps, in_sample_loss))\n",
    "    best_loss = min(best_loss, loss)\n",
    "\n",
    "    for _ in range(500):\n",
    "        minibatch = get_random_subset(chunk, MINIBATCH_SIZE)\n",
    "        working = time.time()\n",
    "        net.train(minibatch, lr)\n",
    "        total_work += time.time() - working\n",
    "        # Try really hard to not keep any views around!\n",
    "        del minibatch\n",
    "        total_training_steps += 1\n",
    "\n",
    "    # Periodically swap out the data for fresh training data.\n",
    "    if (overall_step + 1) % 5 == 0:\n",
    "        load_next_chunk()\n",
    "    if (overall_step + 1) % 150 == 0:\n",
    "        save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_next_chunk()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.sess = sess\n",
    "model.save_model(net, \"model.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [12, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hold(True)\n",
    "plt.plot(*zip(*loss_plot))\n",
    "plt.plot(*zip(*in_sample_loss_plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print [int(chunk[\"features\"][i].sum() == 96) for i in xrange(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print [chunk[\"moves\"][i].sum()+1 for i in xrange(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(chunk[\"moves\"])\n",
    "print len(chunk[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk[\"moves\"][:1000].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (-1, 0, 1):\n",
    "    print i, list(chunk[\"moves\"][:100000]).count(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buckets = {-1: [], 0: [], 1: []}\n",
    "src = chunk\n",
    "#src = cross_val\n",
    "for i in xrange(20):\n",
    "    s = slice(1000 * i, 1000 * (i + 1))\n",
    "    result = net.final_output.eval(feed_dict={\n",
    "        net.input_ph: src[\"features\"][s],\n",
    "        net.desired_output_ph: src[\"moves\"][s],\n",
    "        net.is_training_ph: False,\n",
    "    })\n",
    "    for output, outcome in zip(result, src[\"moves\"][s]):\n",
    "        buckets[outcome[0]].append(output[0])\n",
    "_ = plt.hist(buckets[-1], bins=100, histtype=\"step\")\n",
    "_ = plt.hist(buckets[0], bins=100, histtype=\"step\")\n",
    "_ = plt.hist(buckets[1], bins=100, histtype=\"step\")\n",
    "plt.legend([\"Loss\", \"Draw\", \"Win\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk[\"moves\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val[\"moves\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val[\"features\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(net.W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.moveaxis(chunk[\"features\"][0], -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i += 1\n",
    "print i, cross_val[\"moves\"][i]\n",
    "utils.features_to_board(cross_val[\"features\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = chunk\n",
    "src = cross_val\n",
    "print \"Average outcome:\", np.array(src[\"moves\"]).sum() / float(len(src[\"moves\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ((chunk[\"moves\"].flatten() - 0.052403045049218534) ** 2).sum()\n",
    "v / len(chunk[\"moves\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidence:\n",
    "\n",
    "18 -- obviously won for white\n",
    "37 -- looks plausibly like a draw\n",
    "38 -- looks like a continuation of 37, and is still a draw\n",
    "39 -- same\n",
    "40 -- same\n",
    "41 -- same\n",
    "47 -- looks won for white\n",
    "59 -- looks won for white, but reported as a draw? edit: according to TB, is a draw\n",
    "70 -- looks won for black\n",
    "75 -- looks won for black\n",
    "90 -- looks won for white\n",
    "91 -- looks won for white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding.\n",
    "\n",
    "a\n",
    "\n",
    "a\n",
    "\n",
    "a\n",
    "\n",
    "a\n",
    "\n",
    "a\n",
    "\n",
    "a\n",
    "\n",
    "a\n",
    "\n",
    "a\n",
    "\n",
    "a\n",
    "\n",
    "a\n",
    "\n",
    "a\n",
    "\n",
    "a\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
